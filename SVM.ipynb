{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y6L_q6MSF8lT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "import collections\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "from skimage.transform import resize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SdOdb4DeORf"
      },
      "source": [
        "Extracting the train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E-q1OVSeATf",
        "outputId": "72f977b5-e87d-419f-bbe4-e4ce675add91"
      },
      "outputs": [],
      "source": [
        "TrainFile = \"/content/data.zip\"\n",
        "\n",
        "with ZipFile(TrainFile, 'r') as zip:\n",
        "\tzip.printdir()\n",
        "\tprint('Extracting all the files now...')\n",
        "\tzip.extractall()\n",
        "\tprint('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LAEXUQdeTd7"
      },
      "source": [
        "Haar Cascade function: takes an image finds the faces and returns them cropped \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X5XNsqz2eJca"
      },
      "outputs": [],
      "source": [
        "def haarCascade(img):\n",
        " face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        " eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        " gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        " faces = face_cascade.detectMultiScale(gray, 1.3, 5,minSize=(30, 30))\n",
        " \n",
        " for (x,y,w,h) in faces:\n",
        "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "    roi_color = img[y:y+h, x:x+w]\n",
        "    newimg=np.array(roi_color)\n",
        "    return newimg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "ejV_cCaXbaze",
        "outputId": "9da92320-fa24-481a-dd0a-9c317b9ea7c9"
      },
      "outputs": [],
      "source": [
        "path = \"/content/data/*/*\"\n",
        "arr=[]\n",
        "target_face_size=(388,388)\n",
        "\n",
        "for file in glob.glob(path):\n",
        "  if('jumana' in file):\n",
        "    mytuple=(resize(haarCascade(cv2.imread(file)), target_face_size),'jumana')\n",
        "    arr.append(mytuple)\n",
        "  elif('farida' in file):\n",
        "    mytuple=(resize(haarCascade(cv2.imread(file)), target_face_size),'farida')\n",
        "    arr.append(mytuple)\n",
        "  elif('mazen'in file):\n",
        "    mytuple=(resize(haarCascade(cv2.imread(file)),target_face_size),'mazen')\n",
        "    arr.append(mytuple)\n",
        "  elif('khaled'in file):\n",
        "    mytuple=(resize(haarCascade(cv2.imread(file)),target_face_size),'khaled')\n",
        "    arr.append(mytuple)\n",
        "  elif('kroush'in file):\n",
        "    mytuple=(resize(haarCascade(cv2.imread(file)),target_face_size),'kroush')\n",
        "    arr.append(mytuple)\n",
        "\n",
        "   \n",
        "df = pd.DataFrame(arr,columns=['Image', 'label'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3hB7p_K-hF3",
        "outputId": "ccfee791-d3ee-4b4a-cb5d-8b126b2a3290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n",
            "(388, 388, 3)\n"
          ]
        }
      ],
      "source": [
        "for i in range(18):\n",
        " print(df.Image[i].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj5LwG3RnKDY"
      },
      "source": [
        "orientations:This parameter determines the number of histogram channels in the HOG feature vector.\n",
        "\n",
        "pixels_per_cell: The size of the cell over which to calculate the histograms and determines the size of the spatial binning\n",
        "\n",
        "cells_per_block: The number of cells to include in each block of the HOG feature vector.\n",
        "\n",
        "block_norm:The default is 'L2-Hys' which applies L2 normalization followed by a square-root operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcdwVroRg_i8",
        "outputId": "9ba1352b-5599-4271-b104-06f12715fa4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  50.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from skimage import feature\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "images = df['Image']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "features = []\n",
        "for i in range(len(images)):\n",
        "    # Extracting the HOG features from the images\n",
        "    hog = feature.hog(images[i], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys', channel_axis=2)\n",
        "    features.append(hog)\n",
        "features = np.array(features)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "#SVM model\n",
        "SVM = LinearSVC(random_state=42)\n",
        "SVM.fit(X_train, y_train)\n",
        "\n",
        "# Test SVM classifier\n",
        "accuracy = SVM.score(X_test, y_test)\n",
        "print(\"Accuracy: \",accuracy * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQzpWBbZp2kT"
      },
      "source": [
        "*we try predicting the labels of all images of the df and compare to actual labels*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HetJ4mzCkwRI",
        "outputId": "530ed360-0b6d-4955-801e-41401bc577dc"
      },
      "outputs": [],
      "source": [
        "for i in range(19):\n",
        "  img = df.Image[i]\n",
        "  hog = feature.hog(img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys', channel_axis=2)\n",
        "  hog = hog.reshape(1, -1)\n",
        "  label = SVM.predict(hog)\n",
        "  print(\"Predicted label: \",label)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
